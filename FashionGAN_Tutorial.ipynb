{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64f04b6a-fe54-41ce-92fe-ae94fc587387"
      },
      "source": [
        "# 1. Import Dependencies and Data"
      ],
      "id": "64f04b6a-fe54-41ce-92fe-ae94fc587387"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdfdc6YzYxOW",
        "outputId": "0dcdec19-3e70-46db-b409-a7efeb657a26"
      },
      "id": "fdfdc6YzYxOW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug  8 15:27:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    29W /  70W |  14571MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3c355f3-39f9-4f73-ac4e-5ad62198a580",
        "outputId": "96659bec-aed1-4f9e-8b5e-912341972256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-gpu matplotlib tensorflow-datasets ipywidgets"
      ],
      "id": "c3c355f3-39f9-4f73-ac4e-5ad62198a580"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6112401-2397-423d-b8c2-113a9d323ab0",
        "outputId": "f4134911-2a7b-4166-ed49-baa9cba04ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                          Version\n",
            "-------------------------------- ---------------------\n",
            "absl-py                          1.4.0\n",
            "aiohttp                          3.8.5\n",
            "aiosignal                        1.3.1\n",
            "alabaster                        0.7.13\n",
            "albumentations                   1.2.1\n",
            "altair                           4.2.2\n",
            "anyio                            3.7.1\n",
            "appdirs                          1.4.4\n",
            "argon2-cffi                      21.3.0\n",
            "argon2-cffi-bindings             21.2.0\n",
            "array-record                     0.4.0\n",
            "arviz                            0.15.1\n",
            "astropy                          5.2.2\n",
            "astunparse                       1.6.3\n",
            "async-timeout                    4.0.2\n",
            "attrs                            23.1.0\n",
            "audioread                        3.0.0\n",
            "autograd                         1.6.2\n",
            "Babel                            2.12.1\n",
            "backcall                         0.2.0\n",
            "beautifulsoup4                   4.11.2\n",
            "bleach                           6.0.0\n",
            "blinker                          1.4\n",
            "blis                             0.7.10\n",
            "blosc2                           2.0.0\n",
            "bokeh                            3.1.1\n",
            "branca                           0.6.0\n",
            "build                            0.10.0\n",
            "CacheControl                     0.13.1\n",
            "cachetools                       5.3.1\n",
            "catalogue                        2.0.9\n",
            "certifi                          2023.7.22\n",
            "cffi                             1.15.1\n",
            "chardet                          4.0.0\n",
            "charset-normalizer               2.0.12\n",
            "chex                             0.1.7\n",
            "click                            8.1.6\n",
            "click-plugins                    1.1.1\n",
            "cligj                            0.7.2\n",
            "cloudpickle                      2.2.1\n",
            "cmake                            3.25.2\n",
            "cmdstanpy                        1.1.0\n",
            "colorcet                         3.0.1\n",
            "colorlover                       0.3.0\n",
            "community                        1.0.0b1\n",
            "confection                       0.1.0\n",
            "cons                             0.4.6\n",
            "contextlib2                      21.6.0\n",
            "contourpy                        1.1.0\n",
            "convertdate                      2.4.0\n",
            "cryptography                     3.4.8\n",
            "cufflinks                        0.17.3\n",
            "cupy-cuda11x                     11.0.0\n",
            "cvxopt                           1.3.1\n",
            "cvxpy                            1.3.2\n",
            "cycler                           0.11.0\n",
            "cymem                            2.0.7\n",
            "Cython                           0.29.36\n",
            "dask                             2022.12.1\n",
            "datascience                      0.17.6\n",
            "db-dtypes                        1.1.1\n",
            "dbus-python                      1.2.18\n",
            "debugpy                          1.6.6\n",
            "decorator                        4.4.2\n",
            "defusedxml                       0.7.1\n",
            "distributed                      2022.12.1\n",
            "distro                           1.7.0\n",
            "dlib                             19.24.2\n",
            "dm-tree                          0.1.8\n",
            "docutils                         0.18.1\n",
            "dopamine-rl                      4.0.6\n",
            "duckdb                           0.8.1\n",
            "earthengine-api                  0.1.361\n",
            "easydict                         1.10\n",
            "ecos                             2.0.12\n",
            "editdistance                     0.6.2\n",
            "en-core-web-sm                   3.5.0\n",
            "entrypoints                      0.4\n",
            "ephem                            4.1.4\n",
            "et-xmlfile                       1.1.0\n",
            "etils                            1.4.0\n",
            "etuples                          0.3.9\n",
            "exceptiongroup                   1.1.2\n",
            "fastai                           2.7.12\n",
            "fastcore                         1.5.29\n",
            "fastdownload                     0.0.7\n",
            "fastjsonschema                   2.18.0\n",
            "fastprogress                     1.0.3\n",
            "fastrlock                        0.8.1\n",
            "filelock                         3.12.2\n",
            "Fiona                            1.9.4.post1\n",
            "firebase-admin                   5.3.0\n",
            "Flask                            2.2.5\n",
            "flatbuffers                      23.5.26\n",
            "flax                             0.7.0\n",
            "folium                           0.14.0\n",
            "fonttools                        4.41.1\n",
            "frozendict                       2.3.8\n",
            "frozenlist                       1.4.0\n",
            "fsspec                           2023.6.0\n",
            "future                           0.18.3\n",
            "gast                             0.4.0\n",
            "gcsfs                            2023.6.0\n",
            "GDAL                             3.4.3\n",
            "gdown                            4.6.6\n",
            "gensim                           4.3.1\n",
            "geographiclib                    2.0\n",
            "geopandas                        0.13.2\n",
            "geopy                            2.3.0\n",
            "gin-config                       0.5.0\n",
            "glob2                            0.7\n",
            "google                           2.0.3\n",
            "google-api-core                  2.11.1\n",
            "google-api-python-client         2.84.0\n",
            "google-auth                      2.17.3\n",
            "google-auth-httplib2             0.1.0\n",
            "google-auth-oauthlib             1.0.0\n",
            "google-cloud-bigquery            3.10.0\n",
            "google-cloud-bigquery-connection 1.12.1\n",
            "google-cloud-bigquery-storage    2.22.0\n",
            "google-cloud-core                2.3.3\n",
            "google-cloud-datastore           2.15.2\n",
            "google-cloud-firestore           2.11.1\n",
            "google-cloud-functions           1.13.1\n",
            "google-cloud-language            2.9.1\n",
            "google-cloud-storage             2.8.0\n",
            "google-cloud-translate           3.11.2\n",
            "google-colab                     1.0.0\n",
            "google-crc32c                    1.5.0\n",
            "google-pasta                     0.2.0\n",
            "google-resumable-media           2.5.0\n",
            "googleapis-common-protos         1.59.1\n",
            "googledrivedownloader            0.4\n",
            "graphviz                         0.20.1\n",
            "greenlet                         2.0.2\n",
            "grpc-google-iam-v1               0.12.6\n",
            "grpcio                           1.56.2\n",
            "grpcio-status                    1.48.2\n",
            "gspread                          3.4.2\n",
            "gspread-dataframe                3.3.1\n",
            "gym                              0.25.2\n",
            "gym-notices                      0.0.8\n",
            "h5netcdf                         1.2.0\n",
            "h5py                             3.8.0\n",
            "holidays                         0.29\n",
            "holoviews                        1.15.4\n",
            "html5lib                         1.1\n",
            "httpimport                       1.3.1\n",
            "httplib2                         0.21.0\n",
            "humanize                         4.6.0\n",
            "hyperopt                         0.2.7\n",
            "idna                             3.4\n",
            "imageio                          2.25.1\n",
            "imageio-ffmpeg                   0.4.8\n",
            "imagesize                        1.4.1\n",
            "imbalanced-learn                 0.10.1\n",
            "imgaug                           0.4.0\n",
            "importlib-metadata               4.6.4\n",
            "importlib-resources              6.0.0\n",
            "imutils                          0.5.4\n",
            "inflect                          6.0.5\n",
            "iniconfig                        2.0.0\n",
            "intel-openmp                     2023.2.0\n",
            "ipykernel                        5.5.6\n",
            "ipython                          7.34.0\n",
            "ipython-genutils                 0.2.0\n",
            "ipython-sql                      0.4.1\n",
            "ipywidgets                       7.7.1\n",
            "itsdangerous                     2.1.2\n",
            "jax                              0.4.13\n",
            "jaxlib                           0.4.13+cuda11.cudnn86\n",
            "jeepney                          0.7.1\n",
            "jieba                            0.42.1\n",
            "Jinja2                           3.1.2\n",
            "joblib                           1.3.1\n",
            "jsonpickle                       3.0.1\n",
            "jsonschema                       4.3.3\n",
            "jupyter-client                   6.1.12\n",
            "jupyter-console                  6.1.0\n",
            "jupyter_core                     5.3.1\n",
            "jupyter-server                   1.24.0\n",
            "jupyterlab-pygments              0.2.2\n",
            "jupyterlab-widgets               3.0.8\n",
            "kaggle                           1.5.16\n",
            "keras                            2.12.0\n",
            "keyring                          23.5.0\n",
            "kiwisolver                       1.4.4\n",
            "langcodes                        3.3.0\n",
            "launchpadlib                     1.10.16\n",
            "lazr.restfulclient               0.14.4\n",
            "lazr.uri                         1.0.6\n",
            "lazy_loader                      0.3\n",
            "libclang                         16.0.6\n",
            "librosa                          0.10.0.post2\n",
            "lightgbm                         3.3.5\n",
            "linkify-it-py                    2.0.2\n",
            "lit                              16.0.6\n",
            "llvmlite                         0.39.1\n",
            "locket                           1.0.0\n",
            "logical-unification              0.4.6\n",
            "LunarCalendar                    0.0.9\n",
            "lxml                             4.9.3\n",
            "Markdown                         3.4.4\n",
            "markdown-it-py                   3.0.0\n",
            "MarkupSafe                       2.1.3\n",
            "matplotlib                       3.7.1\n",
            "matplotlib-inline                0.1.6\n",
            "matplotlib-venn                  0.11.9\n",
            "mdit-py-plugins                  0.4.0\n",
            "mdurl                            0.1.2\n",
            "miniKanren                       1.0.3\n",
            "missingno                        0.5.2\n",
            "mistune                          0.8.4\n",
            "mizani                           0.8.1\n",
            "mkl                              2019.0\n",
            "ml-dtypes                        0.2.0\n",
            "mlxtend                          0.22.0\n",
            "more-itertools                   9.1.0\n",
            "moviepy                          1.0.3\n",
            "mpmath                           1.3.0\n",
            "msgpack                          1.0.5\n",
            "multidict                        6.0.4\n",
            "multipledispatch                 1.0.0\n",
            "multitasking                     0.0.11\n",
            "murmurhash                       1.0.9\n",
            "music21                          8.1.0\n",
            "natsort                          8.3.1\n",
            "nbclient                         0.8.0\n",
            "nbconvert                        6.5.4\n",
            "nbformat                         5.9.1\n",
            "nest-asyncio                     1.5.6\n",
            "networkx                         3.1\n",
            "nibabel                          4.0.2\n",
            "nltk                             3.8.1\n",
            "notebook                         6.4.8\n",
            "numba                            0.56.4\n",
            "numexpr                          2.8.4\n",
            "numpy                            1.22.4\n",
            "oauth2client                     4.1.3\n",
            "oauthlib                         3.2.2\n",
            "opencv-contrib-python            4.7.0.72\n",
            "opencv-python                    4.7.0.72\n",
            "opencv-python-headless           4.8.0.74\n",
            "openpyxl                         3.0.10\n",
            "opt-einsum                       3.3.0\n",
            "optax                            0.1.7\n",
            "orbax-checkpoint                 0.3.1\n",
            "osqp                             0.6.2.post8\n",
            "packaging                        23.1\n",
            "palettable                       3.3.3\n",
            "pandas                           1.5.3\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.17.9\n",
            "pandocfilters                    1.5.0\n",
            "panel                            1.2.1\n",
            "param                            1.13.0\n",
            "parso                            0.8.3\n",
            "partd                            1.4.0\n",
            "pathlib                          1.0.1\n",
            "pathy                            0.10.2\n",
            "patsy                            0.5.3\n",
            "pexpect                          4.8.0\n",
            "pickleshare                      0.7.5\n",
            "Pillow                           9.4.0\n",
            "pip                              23.1.2\n",
            "pip-tools                        6.13.0\n",
            "platformdirs                     3.9.1\n",
            "plotly                           5.13.1\n",
            "plotnine                         0.10.1\n",
            "pluggy                           1.2.0\n",
            "polars                           0.17.3\n",
            "pooch                            1.6.0\n",
            "portpicker                       1.5.2\n",
            "prefetch-generator               1.0.3\n",
            "preshed                          3.0.8\n",
            "prettytable                      0.7.2\n",
            "proglog                          0.1.10\n",
            "progressbar2                     4.2.0\n",
            "prometheus-client                0.17.1\n",
            "promise                          2.3\n",
            "prompt-toolkit                   3.0.39\n",
            "prophet                          1.1.4\n",
            "proto-plus                       1.22.3\n",
            "protobuf                         3.20.3\n",
            "psutil                           5.9.5\n",
            "psycopg2                         2.9.6\n",
            "ptyprocess                       0.7.0\n",
            "py-cpuinfo                       9.0.0\n",
            "py4j                             0.10.9.7\n",
            "pyarrow                          9.0.0\n",
            "pyasn1                           0.5.0\n",
            "pyasn1-modules                   0.3.0\n",
            "pycocotools                      2.0.6\n",
            "pycparser                        2.21\n",
            "pyct                             0.5.0\n",
            "pydantic                         1.10.12\n",
            "pydata-google-auth               1.8.1\n",
            "pydot                            1.4.2\n",
            "pydot-ng                         2.0.0\n",
            "pydotplus                        2.0.2\n",
            "PyDrive                          1.3.1\n",
            "pyerfa                           2.0.0.3\n",
            "pygame                           2.5.0\n",
            "Pygments                         2.14.0\n",
            "PyGObject                        3.42.1\n",
            "PyJWT                            2.3.0\n",
            "pymc                             5.1.2\n",
            "PyMeeus                          0.5.12\n",
            "pymystem3                        0.2.0\n",
            "PyOpenGL                         3.1.7\n",
            "pyparsing                        3.1.0\n",
            "pyproj                           3.6.0\n",
            "pyproject_hooks                  1.0.0\n",
            "pyrsistent                       0.19.3\n",
            "PySocks                          1.7.1\n",
            "pytensor                         2.10.1\n",
            "pytest                           7.2.2\n",
            "python-apt                       0.0.0\n",
            "python-dateutil                  2.8.2\n",
            "python-louvain                   0.16\n",
            "python-slugify                   8.0.1\n",
            "python-utils                     3.7.0\n",
            "pytz                             2022.7.1\n",
            "pyviz-comms                      2.3.2\n",
            "PyWavelets                       1.4.1\n",
            "PyYAML                           6.0.1\n",
            "pyzmq                            23.2.1\n",
            "qdldl                            0.1.7.post0\n",
            "qudida                           0.0.4\n",
            "regex                            2022.10.31\n",
            "requests                         2.27.1\n",
            "requests-oauthlib                1.3.1\n",
            "requirements-parser              0.5.0\n",
            "rich                             13.4.2\n",
            "rpy2                             3.4.2\n",
            "rsa                              4.9\n",
            "scikit-image                     0.19.3\n",
            "scikit-learn                     1.2.2\n",
            "scipy                            1.10.1\n",
            "scs                              3.2.3\n",
            "seaborn                          0.12.2\n",
            "SecretStorage                    3.3.1\n",
            "Send2Trash                       1.8.2\n",
            "setuptools                       67.7.2\n",
            "shapely                          2.0.1\n",
            "six                              1.16.0\n",
            "sklearn-pandas                   2.2.0\n",
            "smart-open                       6.3.0\n",
            "sniffio                          1.3.0\n",
            "snowballstemmer                  2.2.0\n",
            "sortedcontainers                 2.4.0\n",
            "soundfile                        0.12.1\n",
            "soupsieve                        2.4.1\n",
            "soxr                             0.3.5\n",
            "spacy                            3.5.4\n",
            "spacy-legacy                     3.0.12\n",
            "spacy-loggers                    1.0.4\n",
            "Sphinx                           5.0.2\n",
            "sphinxcontrib-applehelp          1.0.4\n",
            "sphinxcontrib-devhelp            1.0.2\n",
            "sphinxcontrib-htmlhelp           2.0.1\n",
            "sphinxcontrib-jsmath             1.0.1\n",
            "sphinxcontrib-qthelp             1.0.3\n",
            "sphinxcontrib-serializinghtml    1.1.5\n",
            "SQLAlchemy                       2.0.19\n",
            "sqlparse                         0.4.4\n",
            "srsly                            2.4.7\n",
            "statsmodels                      0.13.5\n",
            "sympy                            1.11.1\n",
            "tables                           3.8.0\n",
            "tabulate                         0.9.0\n",
            "tblib                            2.0.0\n",
            "tenacity                         8.2.2\n",
            "tensorboard                      2.12.3\n",
            "tensorboard-data-server          0.7.1\n",
            "tensorflow                       2.12.0\n",
            "tensorflow-datasets              4.9.2\n",
            "tensorflow-estimator             2.12.0\n",
            "tensorflow-gcs-config            2.12.0\n",
            "tensorflow-hub                   0.14.0\n",
            "tensorflow-io-gcs-filesystem     0.32.0\n",
            "tensorflow-metadata              1.13.1\n",
            "tensorflow-probability           0.20.1\n",
            "tensorstore                      0.1.40\n",
            "termcolor                        2.3.0\n",
            "terminado                        0.17.1\n",
            "text-unidecode                   1.3\n",
            "textblob                         0.17.1\n",
            "tf-slim                          1.1.0\n",
            "thinc                            8.1.10\n",
            "threadpoolctl                    3.2.0\n",
            "tifffile                         2023.7.18\n",
            "tinycss2                         1.2.1\n",
            "toml                             0.10.2\n",
            "tomli                            2.0.1\n",
            "toolz                            0.12.0\n",
            "torch                            2.0.1+cu118\n",
            "torchaudio                       2.0.2+cu118\n",
            "torchdata                        0.6.1\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.15.2\n",
            "torchvision                      0.15.2+cu118\n",
            "tornado                          6.3.1\n",
            "tqdm                             4.65.0\n",
            "traitlets                        5.7.1\n",
            "triton                           2.0.0\n",
            "tweepy                           4.13.0\n",
            "typer                            0.9.0\n",
            "types-setuptools                 68.0.0.3\n",
            "typing_extensions                4.7.1\n",
            "tzlocal                          5.0.1\n",
            "uc-micro-py                      1.0.2\n",
            "uritemplate                      4.1.1\n",
            "urllib3                          1.26.16\n",
            "vega-datasets                    0.9.0\n",
            "wadllib                          1.3.6\n",
            "wasabi                           1.1.2\n",
            "wcwidth                          0.2.6\n",
            "webcolors                        1.13\n",
            "webencodings                     0.5.1\n",
            "websocket-client                 1.6.1\n",
            "Werkzeug                         2.3.6\n",
            "wheel                            0.41.0\n",
            "widgetsnbextension               3.6.4\n",
            "wordcloud                        1.8.2.2\n",
            "wrapt                            1.14.1\n",
            "xarray                           2022.12.0\n",
            "xarray-einstats                  0.6.0\n",
            "xgboost                          1.7.6\n",
            "xlrd                             2.0.1\n",
            "xyzservices                      2023.7.0\n",
            "yarl                             1.9.2\n",
            "yellowbrick                      1.5\n",
            "yfinance                         0.2.25\n",
            "zict                             3.0.0\n",
            "zipp                             3.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ],
      "id": "e6112401-2397-423d-b8c2-113a9d323ab0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b84be907-35e2-43db-a645-b6b164302aaa"
      },
      "outputs": [],
      "source": [
        "# Bringing in tensorflow\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "id": "b84be907-35e2-43db-a645-b6b164302aaa"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD1dXa9zYMQI",
        "outputId": "c29fdc4b-aed4-4681-c3c9-6c57cf0d0b10"
      },
      "id": "TD1dXa9zYMQI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug  8 15:27:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    29W /  70W |  14571MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKwX7WeaYMKl"
      },
      "id": "OKwX7WeaYMKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0f2aa32-064b-448c-bb27-f19a48c40115"
      },
      "outputs": [],
      "source": [
        "# Brining in tensorflow datasets for fashion mnist\n",
        "import tensorflow_datasets as tfds\n",
        "# Bringing in matplotlib for viz stuff\n",
        "from matplotlib import pyplot as plt"
      ],
      "id": "a0f2aa32-064b-448c-bb27-f19a48c40115"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c933f988-d1ee-4d4d-8028-368a158c27e2"
      },
      "outputs": [],
      "source": [
        "# Use the tensorflow datasets api to bring in the data source\n",
        "ds = tfds.load('fashion_mnist', split='train')"
      ],
      "id": "c933f988-d1ee-4d4d-8028-368a158c27e2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c361db0d-8e7b-43e1-97f9-5e3f7cb01ffe",
        "outputId": "04bcde03-8119-4fa1-aaae-9479422407a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "ds.as_numpy_iterator().next()['label']"
      ],
      "id": "c361db0d-8e7b-43e1-97f9-5e3f7cb01ffe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea1635e4-4beb-493d-92c1-b106c806ca70"
      },
      "source": [
        "# 2. Viz Data and Build Dataset"
      ],
      "id": "ea1635e4-4beb-493d-92c1-b106c806ca70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0c62caf-e406-4d12-af31-6f4848155844"
      },
      "outputs": [],
      "source": [
        "# Do some data transformation\n",
        "import numpy as np"
      ],
      "id": "b0c62caf-e406-4d12-af31-6f4848155844"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3215c900-6e85-4b39-b300-ea18faf30e5c"
      },
      "outputs": [],
      "source": [
        "# Setup connection aka iterator\n",
        "dataiterator = ds.as_numpy_iterator()"
      ],
      "id": "3215c900-6e85-4b39-b300-ea18faf30e5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1d6e079-46da-43ca-80d2-b3c864d90360",
        "scrolled": true,
        "outputId": "addad41c-9f3a-4c17-9025-b6c1fa385f5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 18],\n",
              "        [ 77],\n",
              "        [227],\n",
              "        [227],\n",
              "        [208],\n",
              "        [210],\n",
              "        [225],\n",
              "        [216],\n",
              "        [ 85],\n",
              "        [ 32],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 61],\n",
              "        [100],\n",
              "        [ 97],\n",
              "        [ 80],\n",
              "        [ 57],\n",
              "        [117],\n",
              "        [227],\n",
              "        [238],\n",
              "        [115],\n",
              "        [ 49],\n",
              "        [ 78],\n",
              "        [106],\n",
              "        [108],\n",
              "        [ 71],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 81],\n",
              "        [105],\n",
              "        [ 80],\n",
              "        [ 69],\n",
              "        [ 72],\n",
              "        [ 64],\n",
              "        [ 44],\n",
              "        [ 21],\n",
              "        [ 13],\n",
              "        [ 44],\n",
              "        [ 69],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 80],\n",
              "        [114],\n",
              "        [ 80],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 26],\n",
              "        [ 92],\n",
              "        [ 69],\n",
              "        [ 68],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 71],\n",
              "        [ 74],\n",
              "        [ 83],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 78],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 83],\n",
              "        [ 77],\n",
              "        [108],\n",
              "        [ 34],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 55],\n",
              "        [ 92],\n",
              "        [ 69],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 71],\n",
              "        [ 71],\n",
              "        [ 77],\n",
              "        [ 69],\n",
              "        [ 66],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 80],\n",
              "        [ 80],\n",
              "        [ 78],\n",
              "        [ 94],\n",
              "        [ 63],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 63],\n",
              "        [ 95],\n",
              "        [ 66],\n",
              "        [ 68],\n",
              "        [ 72],\n",
              "        [ 72],\n",
              "        [ 69],\n",
              "        [ 72],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 80],\n",
              "        [ 77],\n",
              "        [106],\n",
              "        [ 61],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 80],\n",
              "        [108],\n",
              "        [ 71],\n",
              "        [ 69],\n",
              "        [ 72],\n",
              "        [ 71],\n",
              "        [ 69],\n",
              "        [ 72],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 72],\n",
              "        [ 72],\n",
              "        [ 75],\n",
              "        [ 78],\n",
              "        [ 72],\n",
              "        [ 85],\n",
              "        [128],\n",
              "        [ 64],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 88],\n",
              "        [120],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 72],\n",
              "        [ 77],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 78],\n",
              "        [ 83],\n",
              "        [ 83],\n",
              "        [ 66],\n",
              "        [111],\n",
              "        [123],\n",
              "        [ 78],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 85],\n",
              "        [134],\n",
              "        [ 74],\n",
              "        [ 85],\n",
              "        [ 69],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 81],\n",
              "        [ 75],\n",
              "        [ 61],\n",
              "        [151],\n",
              "        [115],\n",
              "        [ 91],\n",
              "        [ 12],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 10],\n",
              "        [ 85],\n",
              "        [153],\n",
              "        [ 83],\n",
              "        [ 80],\n",
              "        [ 68],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 80],\n",
              "        [ 68],\n",
              "        [ 61],\n",
              "        [162],\n",
              "        [122],\n",
              "        [ 78],\n",
              "        [  6],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 30],\n",
              "        [ 75],\n",
              "        [154],\n",
              "        [ 85],\n",
              "        [ 80],\n",
              "        [ 71],\n",
              "        [ 80],\n",
              "        [ 72],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 78],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 49],\n",
              "        [191],\n",
              "        [132],\n",
              "        [ 72],\n",
              "        [ 15],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 58],\n",
              "        [ 66],\n",
              "        [174],\n",
              "        [115],\n",
              "        [ 66],\n",
              "        [ 77],\n",
              "        [ 80],\n",
              "        [ 72],\n",
              "        [ 78],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 78],\n",
              "        [ 78],\n",
              "        [ 77],\n",
              "        [ 66],\n",
              "        [ 49],\n",
              "        [222],\n",
              "        [131],\n",
              "        [ 77],\n",
              "        [ 37],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 69],\n",
              "        [ 55],\n",
              "        [179],\n",
              "        [139],\n",
              "        [ 55],\n",
              "        [ 92],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 78],\n",
              "        [ 74],\n",
              "        [ 78],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 80],\n",
              "        [ 64],\n",
              "        [ 55],\n",
              "        [242],\n",
              "        [111],\n",
              "        [ 95],\n",
              "        [ 44],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 74],\n",
              "        [ 57],\n",
              "        [159],\n",
              "        [180],\n",
              "        [ 55],\n",
              "        [ 92],\n",
              "        [ 64],\n",
              "        [ 72],\n",
              "        [ 74],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 78],\n",
              "        [ 55],\n",
              "        [ 66],\n",
              "        [255],\n",
              "        [ 97],\n",
              "        [108],\n",
              "        [ 49],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 74],\n",
              "        [ 66],\n",
              "        [145],\n",
              "        [153],\n",
              "        [ 72],\n",
              "        [ 83],\n",
              "        [ 58],\n",
              "        [ 78],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 72],\n",
              "        [ 80],\n",
              "        [ 30],\n",
              "        [132],\n",
              "        [255],\n",
              "        [ 37],\n",
              "        [122],\n",
              "        [ 60],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 80],\n",
              "        [ 69],\n",
              "        [142],\n",
              "        [180],\n",
              "        [142],\n",
              "        [ 57],\n",
              "        [ 64],\n",
              "        [ 78],\n",
              "        [ 74],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 72],\n",
              "        [ 85],\n",
              "        [ 21],\n",
              "        [185],\n",
              "        [227],\n",
              "        [ 37],\n",
              "        [143],\n",
              "        [ 63],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 83],\n",
              "        [ 71],\n",
              "        [136],\n",
              "        [194],\n",
              "        [126],\n",
              "        [ 46],\n",
              "        [ 69],\n",
              "        [ 75],\n",
              "        [ 72],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 78],\n",
              "        [ 38],\n",
              "        [139],\n",
              "        [185],\n",
              "        [ 60],\n",
              "        [151],\n",
              "        [ 58],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  4],\n",
              "        [ 81],\n",
              "        [ 74],\n",
              "        [145],\n",
              "        [177],\n",
              "        [ 78],\n",
              "        [ 49],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 72],\n",
              "        [ 63],\n",
              "        [ 80],\n",
              "        [156],\n",
              "        [117],\n",
              "        [153],\n",
              "        [ 55],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 10],\n",
              "        [ 80],\n",
              "        [ 72],\n",
              "        [157],\n",
              "        [163],\n",
              "        [ 61],\n",
              "        [ 55],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 71],\n",
              "        [ 60],\n",
              "        [ 98],\n",
              "        [156],\n",
              "        [132],\n",
              "        [ 58],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 13],\n",
              "        [ 77],\n",
              "        [ 74],\n",
              "        [157],\n",
              "        [143],\n",
              "        [ 43],\n",
              "        [ 61],\n",
              "        [ 72],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 71],\n",
              "        [ 58],\n",
              "        [ 80],\n",
              "        [157],\n",
              "        [120],\n",
              "        [ 66],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 18],\n",
              "        [ 81],\n",
              "        [ 74],\n",
              "        [156],\n",
              "        [114],\n",
              "        [ 35],\n",
              "        [ 72],\n",
              "        [ 71],\n",
              "        [ 75],\n",
              "        [ 78],\n",
              "        [ 72],\n",
              "        [ 66],\n",
              "        [ 80],\n",
              "        [ 78],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 64],\n",
              "        [ 63],\n",
              "        [165],\n",
              "        [119],\n",
              "        [ 68],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 23],\n",
              "        [ 85],\n",
              "        [ 81],\n",
              "        [177],\n",
              "        [ 57],\n",
              "        [ 52],\n",
              "        [ 77],\n",
              "        [ 71],\n",
              "        [ 78],\n",
              "        [ 80],\n",
              "        [ 72],\n",
              "        [ 75],\n",
              "        [ 74],\n",
              "        [ 77],\n",
              "        [ 77],\n",
              "        [ 75],\n",
              "        [ 64],\n",
              "        [ 37],\n",
              "        [173],\n",
              "        [ 95],\n",
              "        [ 72],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 26],\n",
              "        [ 81],\n",
              "        [ 86],\n",
              "        [160],\n",
              "        [ 20],\n",
              "        [ 75],\n",
              "        [ 77],\n",
              "        [ 77],\n",
              "        [ 80],\n",
              "        [ 78],\n",
              "        [ 80],\n",
              "        [ 89],\n",
              "        [ 78],\n",
              "        [ 81],\n",
              "        [ 83],\n",
              "        [ 80],\n",
              "        [ 74],\n",
              "        [ 20],\n",
              "        [177],\n",
              "        [ 77],\n",
              "        [ 74],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 49],\n",
              "        [ 77],\n",
              "        [ 91],\n",
              "        [200],\n",
              "        [  0],\n",
              "        [ 83],\n",
              "        [ 95],\n",
              "        [ 86],\n",
              "        [ 88],\n",
              "        [ 88],\n",
              "        [ 89],\n",
              "        [ 88],\n",
              "        [ 89],\n",
              "        [ 88],\n",
              "        [ 83],\n",
              "        [ 89],\n",
              "        [ 86],\n",
              "        [  0],\n",
              "        [191],\n",
              "        [ 78],\n",
              "        [ 80],\n",
              "        [ 24],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 54],\n",
              "        [ 71],\n",
              "        [108],\n",
              "        [165],\n",
              "        [  0],\n",
              "        [ 24],\n",
              "        [ 57],\n",
              "        [ 52],\n",
              "        [ 57],\n",
              "        [ 60],\n",
              "        [ 60],\n",
              "        [ 60],\n",
              "        [ 63],\n",
              "        [ 63],\n",
              "        [ 77],\n",
              "        [ 89],\n",
              "        [ 52],\n",
              "        [  0],\n",
              "        [211],\n",
              "        [ 97],\n",
              "        [ 77],\n",
              "        [ 61],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 68],\n",
              "        [ 91],\n",
              "        [117],\n",
              "        [137],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 18],\n",
              "        [216],\n",
              "        [ 94],\n",
              "        [ 97],\n",
              "        [ 57],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 54],\n",
              "        [115],\n",
              "        [105],\n",
              "        [185],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  1],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [153],\n",
              "        [ 78],\n",
              "        [106],\n",
              "        [ 37],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]],\n",
              "\n",
              "       [[  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [ 18],\n",
              "        [ 61],\n",
              "        [ 41],\n",
              "        [103],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [106],\n",
              "        [ 47],\n",
              "        [ 69],\n",
              "        [ 23],\n",
              "        [  0],\n",
              "        [  0],\n",
              "        [  0]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Getting data out of the pipeline\n",
        "dataiterator.next()['image']"
      ],
      "id": "c1d6e079-46da-43ca-80d2-b3c864d90360"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "deb5fca0-fd8a-4557-9c72-1a60c289a2e5",
        "outputId": "4672a8ec-971a-451e-8fd6-81759dceb5e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setup the subplot formatting\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "# Loop four times and get images\n",
        "for idx in range(4):\n",
        "    # Grab an image and label\n",
        "    sample = dataiterator.next()\n",
        "    # Plot the image using a specific subplot\n",
        "    ax[idx].imshow(np.squeeze(sample['image']))\n",
        "    # Appending the image label as the plot title\n",
        "    ax[idx].title.set_text(sample['label'])"
      ],
      "id": "deb5fca0-fd8a-4557-9c72-1a60c289a2e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66c9d901-6a5c-42fd-ad06-cc03f7829728"
      },
      "outputs": [],
      "source": [
        "# Scale and return images only\n",
        "def scale_images(data):\n",
        "    image = data['image']\n",
        "    return image / 255"
      ],
      "id": "66c9d901-6a5c-42fd-ad06-cc03f7829728"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfc9b6b1-e06e-421c-9c5c-bfc3b3e3be77"
      },
      "outputs": [],
      "source": [
        "# Reload the dataset\n",
        "ds = tfds.load('fashion_mnist', split='train')\n",
        "# Running the dataset through the scale_images preprocessing step\n",
        "ds = ds.map(scale_images)\n",
        "# Cache the dataset for that batch\n",
        "ds = ds.cache()\n",
        "# Shuffle it up\n",
        "ds = ds.shuffle(60000)\n",
        "# Batch into 128 images per sample\n",
        "ds = ds.batch(8000)\n",
        "# Reduces the likelihood of bottlenecking\n",
        "ds = ds.prefetch(64)"
      ],
      "id": "dfc9b6b1-e06e-421c-9c5c-bfc3b3e3be77"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofEzzEshv-ds",
        "outputId": "6506a306-5034-4c5f-ba60-b72c0d73524c"
      },
      "id": "ofEzzEshv-ds",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbb52952-faa1-445f-8931-2f0f37224bfb",
        "outputId": "9e270e95-4bd4-44e8-91bc-24f3cd12e830"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "ds.as_numpy_iterator().next().shape"
      ],
      "id": "fbb52952-faa1-445f-8931-2f0f37224bfb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5b08df-7b20-41f4-a8ff-112dface1cb0"
      },
      "source": [
        "# 3. Build Neural Network"
      ],
      "id": "9a5b08df-7b20-41f4-a8ff-112dface1cb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38f66add-a3db-467f-96c3-f87b9f880159"
      },
      "source": [
        "### 3.1 Import Modelling Components"
      ],
      "id": "38f66add-a3db-467f-96c3-f87b9f880159"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb72da39-377f-4264-b525-c87f49fb0356"
      },
      "outputs": [],
      "source": [
        "# Bring in the sequential api for the generator and discriminator\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Bring in the layers for the neural network\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D"
      ],
      "id": "bb72da39-377f-4264-b525-c87f49fb0356"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c40405df-1439-4661-8785-d76698df8152"
      },
      "source": [
        "### 3.2 Build Generator"
      ],
      "id": "c40405df-1439-4661-8785-d76698df8152"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d29d43a-e02a-4031-a0ec-de8aa810c118"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Takes in random values and reshapes it to 7x7x128\n",
        "    # Beginnings of a generated image\n",
        "    model.add(Dense(7*7*128, input_dim=128))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Reshape((7,7,128)))\n",
        "\n",
        "    # Upsampling block 1\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, 5, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Upsampling block 2\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, 5, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Convolutional block 1\n",
        "    model.add(Conv2D(128, 4, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Convolutional block 2\n",
        "    model.add(Conv2D(128, 4, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv layer to get to one channel\n",
        "    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "id": "5d29d43a-e02a-4031-a0ec-de8aa810c118"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "741b0d58-1b9f-4260-8405-dc400c73f843"
      },
      "outputs": [],
      "source": [
        "generator = build_generator()"
      ],
      "id": "741b0d58-1b9f-4260-8405-dc400c73f843"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "259ab9c1-6d6c-49a0-b0c4-f45b7c68f588",
        "scrolled": true,
        "outputId": "7c646dea-bf4a-4f31-cfca-ff1a61b639c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 6272)              809088    \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 6272)              0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 14, 14, 128)       409728    \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 28, 28, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 28, 28, 128)       409728    \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 28, 28, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 28, 28, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_13 (LeakyReLU)  (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 28, 28, 1)         2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,155,137\n",
            "Trainable params: 2,155,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generator.summary()"
      ],
      "id": "259ab9c1-6d6c-49a0-b0c4-f45b7c68f588"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ba4d1c-6a15-4097-bf63-5fe6ddb404b6",
        "outputId": "523a8127-4d3d-4528-e259-6d3342fbeb2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 185ms/step\n"
          ]
        }
      ],
      "source": [
        "img = generator.predict(np.random.randn(4,128,1))"
      ],
      "id": "10ba4d1c-6a15-4097-bf63-5fe6ddb404b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b4e0cb6-d741-4d43-b845-2a8f2615765b"
      },
      "outputs": [],
      "source": [
        "# # Generate new fashion\n",
        "# img = generator.predict(np.random.randn(4,128,1))\n",
        "# # Setup the subplot formatting\n",
        "# fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "# # Loop four times and get images\n",
        "# for idx, img in enumerate(img):\n",
        "#     # Plot the image using a specific subplot\n",
        "#     ax[idx].imshow(np.squeeze(img))\n",
        "#     # Appending the image label as the plot title\n",
        "#     ax[idx].title.set_text(idx)"
      ],
      "id": "9b4e0cb6-d741-4d43-b845-2a8f2615765b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2415abbf-24ed-4bac-8fb8-12c65017ec22"
      },
      "source": [
        "### 3.3 Build Discriminator"
      ],
      "id": "2415abbf-24ed-4bac-8fb8-12c65017ec22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4e70bcb-cfd5-42bb-aed0-79f19bb38d17"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "\n",
        "    # First Conv Block\n",
        "    model.add(Conv2D(32, 5, input_shape = (28,28,1)))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Second Conv Block\n",
        "    model.add(Conv2D(64, 5))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Third Conv Block\n",
        "    model.add(Conv2D(128, 5))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Fourth Conv Block\n",
        "    model.add(Conv2D(256, 5))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Flatten then pass to dense layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "id": "b4e70bcb-cfd5-42bb-aed0-79f19bb38d17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7173eb57-250b-4d21-9b37-de842c4552ac"
      },
      "outputs": [],
      "source": [
        "discriminator = build_discriminator()"
      ],
      "id": "7173eb57-250b-4d21-9b37-de842c4552ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed6fecbc-f214-4f50-865c-91887b2430e7",
        "scrolled": true,
        "outputId": "7930068b-936f-45c5-8007-5a333e74c76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          (None, 24, 24, 32)        832       \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 20, 20, 64)        51264     \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 20, 20, 64)        0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 20, 20, 64)        0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 12, 12, 256)       819456    \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 12, 12, 256)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 36864)             0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 36864)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 36865     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,113,345\n",
            "Trainable params: 1,113,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator.summary()"
      ],
      "id": "ed6fecbc-f214-4f50-865c-91887b2430e7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19e32424-f9c5-499c-a13f-b450bc525bdc"
      },
      "outputs": [],
      "source": [
        "# img = img[0]"
      ],
      "id": "19e32424-f9c5-499c-a13f-b450bc525bdc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ce3acc9-02c8-468f-915a-0efd52da0bad",
        "outputId": "12d76272-2449-486b-bd88-6fd3d83b6de3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "img.shape"
      ],
      "id": "9ce3acc9-02c8-468f-915a-0efd52da0bad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cd15246-b40c-4c7a-912d-b88a1c5c463b",
        "outputId": "e00a38f2-5906-4436-c6e1-c0b2c9907b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 125ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49296513],\n",
              "       [0.4932616 ],\n",
              "       [0.49320742],\n",
              "       [0.49326032]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "discriminator.predict(img)"
      ],
      "id": "8cd15246-b40c-4c7a-912d-b88a1c5c463b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b343b0-38d3-4281-bedb-72099a18097e"
      },
      "source": [
        "# 4. Construct Training Loop"
      ],
      "id": "39b343b0-38d3-4281-bedb-72099a18097e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "884abab3-2f74-442d-856f-e104ef1ac8ef"
      },
      "source": [
        "### 4.1 Setup Losses and Optimizers"
      ],
      "id": "884abab3-2f74-442d-856f-e104ef1ac8ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bb1d23a-ea68-451a-bb38-e7795dc24311"
      },
      "outputs": [],
      "source": [
        "# Adam is going to be the optimizer for both\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Binary cross entropy is going to be the loss for both\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ],
      "id": "0bb1d23a-ea68-451a-bb38-e7795dc24311"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "198b2d4e-d6b9-4b6c-a98c-65cd1b81da26"
      },
      "outputs": [],
      "source": [
        "g_opt = Adam(learning_rate=0.0001)\n",
        "d_opt = Adam(learning_rate=0.00001)\n",
        "g_loss = BinaryCrossentropy()\n",
        "d_loss = BinaryCrossentropy()"
      ],
      "id": "198b2d4e-d6b9-4b6c-a98c-65cd1b81da26"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f170b0e-f731-4cbd-8068-24896f462c08"
      },
      "source": [
        "### 4.2 Build Subclassed Model"
      ],
      "id": "9f170b0e-f731-4cbd-8068-24896f462c08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e2f5654-ed22-462d-be32-6c43d8b99b74"
      },
      "outputs": [],
      "source": [
        "# Importing the base model class to subclass our training step\n",
        "from tensorflow.keras.models import Model"
      ],
      "id": "9e2f5654-ed22-462d-be32-6c43d8b99b74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40a0af46-0243-4396-94d6-c1316d984de9"
      },
      "outputs": [],
      "source": [
        "class FashionGAN(Model):\n",
        "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
        "        # Pass through args and kwargs to base class\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Create attributes for gen and disc\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs):\n",
        "        # Compile with base class\n",
        "        super().compile(*args, **kwargs)\n",
        "\n",
        "        # Create attributes for losses and optimizers\n",
        "        self.g_opt = g_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.g_loss = g_loss\n",
        "        self.d_loss = d_loss\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        # Get the data\n",
        "        real_images = batch\n",
        "        fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            # Pass the real and fake images to the discriminator model\n",
        "            yhat_real = self.discriminator(real_images, training=True)\n",
        "            yhat_fake = self.discriminator(fake_images, training=True)\n",
        "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
        "\n",
        "            # Create labels for real and fakes images\n",
        "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
        "\n",
        "            # Add some noise to the TRUE outputs\n",
        "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
        "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
        "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
        "\n",
        "            # Calculate loss - BINARYCROSS\n",
        "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
        "\n",
        "        # Apply backpropagation - nn learn\n",
        "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Train the generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            # Generate some new images\n",
        "            gen_images = self.generator(tf.random.normal((128,128,1)), training=True)\n",
        "\n",
        "            # Create the predicted labels\n",
        "            predicted_labels = self.discriminator(gen_images, training=False)\n",
        "\n",
        "            # Calculate loss - trick to training to fake out the discriminator\n",
        "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels)\n",
        "\n",
        "        # Apply backprop\n",
        "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
      ],
      "id": "40a0af46-0243-4396-94d6-c1316d984de9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24d248c3-f4c1-4478-a699-a5811a7b1fd0"
      },
      "outputs": [],
      "source": [
        "# Create instance of subclassed model\n",
        "fashgan = FashionGAN(generator, discriminator)"
      ],
      "id": "24d248c3-f4c1-4478-a699-a5811a7b1fd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1cf7e02-ee1a-4901-bdf0-9aa2301f8cfc"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "fashgan.compile(g_opt, d_opt, g_loss, d_loss)"
      ],
      "id": "e1cf7e02-ee1a-4901-bdf0-9aa2301f8cfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06d0adb-38d0-4558-b824-7416cf880082"
      },
      "source": [
        "### 4.3 Build Callback"
      ],
      "id": "e06d0adb-38d0-4558-b824-7416cf880082"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "548f6918-366c-4799-9dac-1acedaab40c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.callbacks import Callback"
      ],
      "id": "548f6918-366c-4799-9dac-1acedaab40c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3e2bb77-2d7d-40d0-809f-526b8fd34170"
      },
      "outputs": [],
      "source": [
        "class ModelMonitor(Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = array_to_img(generated_images[i])\n",
        "            img.save(os.path.join('/content/drive/MyDrive/GAN_Image_Output', f'generated_img_{epoch}_{i}.png'))"
      ],
      "id": "d3e2bb77-2d7d-40d0-809f-526b8fd34170"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16e2f159-25e7-4e35-95ef-f0fd18ac5897"
      },
      "source": [
        "### 4.3 Train"
      ],
      "id": "16e2f159-25e7-4e35-95ef-f0fd18ac5897"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a779dceb-aba6-4bf3-af49-0d32a76dd2f7",
        "scrolled": true,
        "outputId": "08a7fc0a-8438-4998-fa2a-81f7e7eab0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "6/8 [=====================>........] - ETA: 6s - d_loss: 0.6644 - g_loss: 0.6462"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.8544s vs `on_train_batch_end` time: 1.7908s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 31s 3s/step - d_loss: 0.6537 - g_loss: 0.6234\n",
            "Epoch 2/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.5891 - g_loss: 0.5011\n",
            "Epoch 3/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.5229 - g_loss: 0.3849\n",
            "Epoch 4/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.4575 - g_loss: 0.2614\n",
            "Epoch 5/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.4021 - g_loss: 0.1516\n",
            "Epoch 6/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3711 - g_loss: 0.0773\n",
            "Epoch 7/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3624 - g_loss: 0.0430\n",
            "Epoch 8/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3635 - g_loss: 0.0346\n",
            "Epoch 9/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3615 - g_loss: 0.0395\n",
            "Epoch 10/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3587 - g_loss: 0.0499\n",
            "Epoch 11/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3582 - g_loss: 0.0593\n",
            "Epoch 12/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3573 - g_loss: 0.0641\n",
            "Epoch 13/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3557 - g_loss: 0.0654\n",
            "Epoch 14/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3551 - g_loss: 0.0665\n",
            "Epoch 15/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3530 - g_loss: 0.0690\n",
            "Epoch 16/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3515 - g_loss: 0.0732\n",
            "Epoch 17/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3499 - g_loss: 0.0787\n",
            "Epoch 18/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3496 - g_loss: 0.0847\n",
            "Epoch 19/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3476 - g_loss: 0.0912\n",
            "Epoch 20/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3466 - g_loss: 0.0977\n",
            "Epoch 21/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3459 - g_loss: 0.1050\n",
            "Epoch 22/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3429 - g_loss: 0.1141\n",
            "Epoch 23/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3420 - g_loss: 0.1244\n",
            "Epoch 24/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3406 - g_loss: 0.1358\n",
            "Epoch 25/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3393 - g_loss: 0.1488\n",
            "Epoch 26/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3368 - g_loss: 0.1642\n",
            "Epoch 27/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3355 - g_loss: 0.1806\n",
            "Epoch 28/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3335 - g_loss: 0.1985\n",
            "Epoch 29/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3326 - g_loss: 0.2183\n",
            "Epoch 30/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3300 - g_loss: 0.2390\n",
            "Epoch 31/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3285 - g_loss: 0.2622\n",
            "Epoch 32/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3270 - g_loss: 0.2878\n",
            "Epoch 33/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3240 - g_loss: 0.3139\n",
            "Epoch 34/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3221 - g_loss: 0.3396\n",
            "Epoch 35/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3208 - g_loss: 0.3672\n",
            "Epoch 36/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3184 - g_loss: 0.3964\n",
            "Epoch 37/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3177 - g_loss: 0.4239\n",
            "Epoch 38/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3163 - g_loss: 0.4498\n",
            "Epoch 39/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3159 - g_loss: 0.4740\n",
            "Epoch 40/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3149 - g_loss: 0.4987\n",
            "Epoch 41/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3132 - g_loss: 0.5195\n",
            "Epoch 42/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3129 - g_loss: 0.5377\n",
            "Epoch 43/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3122 - g_loss: 0.5553\n",
            "Epoch 44/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3112 - g_loss: 0.5696\n",
            "Epoch 45/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3116 - g_loss: 0.5829\n",
            "Epoch 46/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3108 - g_loss: 0.5944\n",
            "Epoch 47/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3087 - g_loss: 0.5169\n",
            "Epoch 48/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3032 - g_loss: 0.2664\n",
            "Epoch 49/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2990 - g_loss: 0.2939\n",
            "Epoch 50/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2960 - g_loss: 0.3300\n",
            "Epoch 51/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2942 - g_loss: 0.3555\n",
            "Epoch 52/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2928 - g_loss: 0.3760\n",
            "Epoch 53/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2925 - g_loss: 0.3938\n",
            "Epoch 54/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2917 - g_loss: 0.4065\n",
            "Epoch 55/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2905 - g_loss: 0.4154\n",
            "Epoch 56/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2905 - g_loss: 0.4235\n",
            "Epoch 57/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2911 - g_loss: 0.4304\n",
            "Epoch 58/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2906 - g_loss: 0.4355\n",
            "Epoch 59/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2904 - g_loss: 0.4395\n",
            "Epoch 60/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2908 - g_loss: 0.4438\n",
            "Epoch 61/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2908 - g_loss: 0.4475\n",
            "Epoch 62/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2899 - g_loss: 0.4501\n",
            "Epoch 63/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2894 - g_loss: 0.4532\n",
            "Epoch 64/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2906 - g_loss: 0.4563\n",
            "Epoch 65/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2895 - g_loss: 0.4588\n",
            "Epoch 66/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2899 - g_loss: 0.4615\n",
            "Epoch 67/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2895 - g_loss: 0.4636\n",
            "Epoch 68/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2895 - g_loss: 0.4657\n",
            "Epoch 69/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2909 - g_loss: 0.4680\n",
            "Epoch 70/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2905 - g_loss: 0.4703\n",
            "Epoch 71/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2886 - g_loss: 0.4718\n",
            "Epoch 72/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2894 - g_loss: 0.4734\n",
            "Epoch 73/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2894 - g_loss: 0.4757\n",
            "Epoch 74/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2895 - g_loss: 0.4778\n",
            "Epoch 75/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2897 - g_loss: 0.4791\n",
            "Epoch 76/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2895 - g_loss: 0.4810\n",
            "Epoch 77/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2903 - g_loss: 0.4836\n",
            "Epoch 78/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2898 - g_loss: 0.4846\n",
            "Epoch 79/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2899 - g_loss: 0.4870\n",
            "Epoch 80/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2890 - g_loss: 0.4890\n",
            "Epoch 81/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2891 - g_loss: 0.4906\n",
            "Epoch 82/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2893 - g_loss: 0.4930\n",
            "Epoch 83/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2905 - g_loss: 0.4948\n",
            "Epoch 84/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2893 - g_loss: 0.4971\n",
            "Epoch 85/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2893 - g_loss: 0.4989\n",
            "Epoch 86/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2885 - g_loss: 0.5009\n",
            "Epoch 87/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2884 - g_loss: 0.5033\n",
            "Epoch 88/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2887 - g_loss: 0.5053\n",
            "Epoch 89/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2881 - g_loss: 0.5074\n",
            "Epoch 90/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2882 - g_loss: 0.5096\n",
            "Epoch 91/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2877 - g_loss: 0.5116\n",
            "Epoch 92/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2881 - g_loss: 0.5141\n",
            "Epoch 93/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2879 - g_loss: 0.5159\n",
            "Epoch 94/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2875 - g_loss: 0.5182\n",
            "Epoch 95/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2875 - g_loss: 0.5205\n",
            "Epoch 96/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2878 - g_loss: 0.5225\n",
            "Epoch 97/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2877 - g_loss: 0.5248\n",
            "Epoch 98/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2872 - g_loss: 0.5266\n",
            "Epoch 99/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2878 - g_loss: 0.5290\n",
            "Epoch 100/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2866 - g_loss: 0.5311\n",
            "Epoch 101/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2873 - g_loss: 0.5336\n",
            "Epoch 102/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2884 - g_loss: 0.5358\n",
            "Epoch 103/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2880 - g_loss: 0.5384\n",
            "Epoch 104/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2873 - g_loss: 0.5402\n",
            "Epoch 105/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2873 - g_loss: 0.5423\n",
            "Epoch 106/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2883 - g_loss: 0.5449\n",
            "Epoch 107/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2869 - g_loss: 0.5469\n",
            "Epoch 108/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2869 - g_loss: 0.5488\n",
            "Epoch 109/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2868 - g_loss: 0.5515\n",
            "Epoch 110/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2866 - g_loss: 0.5533\n",
            "Epoch 111/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2866 - g_loss: 0.5556\n",
            "Epoch 112/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2863 - g_loss: 0.5575\n",
            "Epoch 113/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2871 - g_loss: 0.5592\n",
            "Epoch 114/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2865 - g_loss: 0.5612\n",
            "Epoch 115/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2873 - g_loss: 0.5629\n",
            "Epoch 116/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2866 - g_loss: 0.5650\n",
            "Epoch 117/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2869 - g_loss: 0.5666\n",
            "Epoch 118/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2862 - g_loss: 0.5687\n",
            "Epoch 119/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2861 - g_loss: 0.5698\n",
            "Epoch 120/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2859 - g_loss: 0.5716\n",
            "Epoch 121/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2861 - g_loss: 0.5734\n",
            "Epoch 122/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2858 - g_loss: 0.5747\n",
            "Epoch 123/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2862 - g_loss: 0.5766\n",
            "Epoch 124/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2859 - g_loss: 0.5782\n",
            "Epoch 125/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2863 - g_loss: 0.5797\n",
            "Epoch 126/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2855 - g_loss: 0.5815\n",
            "Epoch 127/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2855 - g_loss: 0.5830\n",
            "Epoch 128/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2855 - g_loss: 0.5843\n",
            "Epoch 129/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2860 - g_loss: 0.5860\n",
            "Epoch 130/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2854 - g_loss: 0.5873\n",
            "Epoch 131/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2855 - g_loss: 0.5889\n",
            "Epoch 132/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2855 - g_loss: 0.5902\n",
            "Epoch 133/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2859 - g_loss: 0.5916\n",
            "Epoch 134/2000\n",
            "8/8 [==============================] - 21s 3s/step - d_loss: 0.2849 - g_loss: 0.5926\n",
            "Epoch 135/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2853 - g_loss: 0.5940\n",
            "Epoch 136/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2851 - g_loss: 0.5949\n",
            "Epoch 137/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2854 - g_loss: 0.5964\n",
            "Epoch 138/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2858 - g_loss: 0.5974\n",
            "Epoch 139/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2849 - g_loss: 0.5987\n",
            "Epoch 140/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2853 - g_loss: 0.5999\n",
            "Epoch 141/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2858 - g_loss: 0.6012\n",
            "Epoch 142/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2849 - g_loss: 0.6022\n",
            "Epoch 143/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2849 - g_loss: 0.6033\n",
            "Epoch 144/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2857 - g_loss: 0.6045\n",
            "Epoch 145/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2846 - g_loss: 0.6056\n",
            "Epoch 146/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2856 - g_loss: 0.6066\n",
            "Epoch 147/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2854 - g_loss: 0.6076\n",
            "Epoch 148/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2849 - g_loss: 0.6086\n",
            "Epoch 149/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2851 - g_loss: 0.6098\n",
            "Epoch 150/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2859 - g_loss: 0.6110\n",
            "Epoch 151/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2852 - g_loss: 0.6120\n",
            "Epoch 152/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2838 - g_loss: 0.6131\n",
            "Epoch 153/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2840 - g_loss: 0.6140\n",
            "Epoch 154/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2854 - g_loss: 0.6152\n",
            "Epoch 155/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2835 - g_loss: 0.6161\n",
            "Epoch 156/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2850 - g_loss: 0.6173\n",
            "Epoch 157/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2842 - g_loss: 0.6181\n",
            "Epoch 158/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2849 - g_loss: 0.6193\n",
            "Epoch 159/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2845 - g_loss: 0.6201\n",
            "Epoch 160/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2851 - g_loss: 0.6212\n",
            "Epoch 161/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2841 - g_loss: 0.6221\n",
            "Epoch 162/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2842 - g_loss: 0.6232\n",
            "Epoch 163/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2849 - g_loss: 0.6241\n",
            "Epoch 164/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2831 - g_loss: 0.6251\n",
            "Epoch 165/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2845 - g_loss: 0.6261\n",
            "Epoch 166/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2840 - g_loss: 0.6272\n",
            "Epoch 167/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2837 - g_loss: 0.6282\n",
            "Epoch 168/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2832 - g_loss: 0.6292\n",
            "Epoch 169/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2837 - g_loss: 0.6304\n",
            "Epoch 170/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2845 - g_loss: 0.6314\n",
            "Epoch 171/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2839 - g_loss: 0.6324\n",
            "Epoch 172/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2839 - g_loss: 0.6336\n",
            "Epoch 173/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2836 - g_loss: 0.6346\n",
            "Epoch 174/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2843 - g_loss: 0.6355\n",
            "Epoch 175/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2835 - g_loss: 0.6364\n",
            "Epoch 176/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2833 - g_loss: 0.6373\n",
            "Epoch 177/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2833 - g_loss: 0.6383\n",
            "Epoch 178/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2830 - g_loss: 0.6393\n",
            "Epoch 179/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2837 - g_loss: 0.6404\n",
            "Epoch 180/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2826 - g_loss: 0.6415\n",
            "Epoch 181/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2825 - g_loss: 0.6424\n",
            "Epoch 182/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2832 - g_loss: 0.6433\n",
            "Epoch 183/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2827 - g_loss: 0.6441\n",
            "Epoch 184/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2820 - g_loss: 0.6449\n",
            "Epoch 185/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2823 - g_loss: 0.6459\n",
            "Epoch 186/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2831 - g_loss: 0.6469\n",
            "Epoch 187/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2823 - g_loss: 0.6479\n",
            "Epoch 188/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2822 - g_loss: 0.6489\n",
            "Epoch 189/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2826 - g_loss: 0.6499\n",
            "Epoch 190/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2828 - g_loss: 0.6510\n",
            "Epoch 191/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2820 - g_loss: 0.6521\n",
            "Epoch 192/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2818 - g_loss: 0.6531\n",
            "Epoch 193/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2824 - g_loss: 0.6541\n",
            "Epoch 194/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2822 - g_loss: 0.6553\n",
            "Epoch 195/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2827 - g_loss: 0.6564\n",
            "Epoch 196/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2821 - g_loss: 0.6576\n",
            "Epoch 197/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2821 - g_loss: 0.6589\n",
            "Epoch 198/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2807 - g_loss: 0.6601\n",
            "Epoch 199/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2809 - g_loss: 0.6615\n",
            "Epoch 200/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2809 - g_loss: 0.6627\n",
            "Epoch 201/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2812 - g_loss: 0.6639\n",
            "Epoch 202/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2813 - g_loss: 0.6652\n",
            "Epoch 203/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2820 - g_loss: 0.6665\n",
            "Epoch 204/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2812 - g_loss: 0.6679\n",
            "Epoch 205/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2808 - g_loss: 0.6692\n",
            "Epoch 206/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2805 - g_loss: 0.6703\n",
            "Epoch 207/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2810 - g_loss: 0.6716\n",
            "Epoch 208/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2811 - g_loss: 0.6728\n",
            "Epoch 209/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2812 - g_loss: 0.6739\n",
            "Epoch 210/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2811 - g_loss: 0.6750\n",
            "Epoch 211/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2805 - g_loss: 0.6760\n",
            "Epoch 212/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2798 - g_loss: 0.6770\n",
            "Epoch 213/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2792 - g_loss: 0.6781\n",
            "Epoch 214/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2790 - g_loss: 0.6792\n",
            "Epoch 215/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2793 - g_loss: 0.6803\n",
            "Epoch 216/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2799 - g_loss: 0.6813\n",
            "Epoch 217/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2799 - g_loss: 0.6823\n",
            "Epoch 218/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2799 - g_loss: 0.6834\n",
            "Epoch 219/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2793 - g_loss: 0.6844\n",
            "Epoch 220/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2784 - g_loss: 0.6854\n",
            "Epoch 221/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2793 - g_loss: 0.6865\n",
            "Epoch 222/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2796 - g_loss: 0.6877\n",
            "Epoch 223/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2792 - g_loss: 0.6889\n",
            "Epoch 224/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2798 - g_loss: 0.6900\n",
            "Epoch 225/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2790 - g_loss: 0.6913\n",
            "Epoch 226/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2787 - g_loss: 0.6926\n",
            "Epoch 227/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2793 - g_loss: 0.6940\n",
            "Epoch 228/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2780 - g_loss: 0.6955\n",
            "Epoch 229/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2791 - g_loss: 0.6970\n",
            "Epoch 230/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2795 - g_loss: 0.6986\n",
            "Epoch 231/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2790 - g_loss: 0.7003\n",
            "Epoch 232/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2790 - g_loss: 0.7022\n",
            "Epoch 233/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2794 - g_loss: 0.7042\n",
            "Epoch 234/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2785 - g_loss: 0.7062\n",
            "Epoch 235/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2790 - g_loss: 0.7084\n",
            "Epoch 236/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2787 - g_loss: 0.7108\n",
            "Epoch 237/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2786 - g_loss: 0.7134\n",
            "Epoch 238/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2789 - g_loss: 0.7162\n",
            "Epoch 239/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2782 - g_loss: 0.7193\n",
            "Epoch 240/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2780 - g_loss: 0.7227\n",
            "Epoch 241/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2778 - g_loss: 0.7265\n",
            "Epoch 242/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2781 - g_loss: 0.7306\n",
            "Epoch 243/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2781 - g_loss: 0.7351\n",
            "Epoch 244/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2770 - g_loss: 0.7402\n",
            "Epoch 245/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2779 - g_loss: 0.7458\n",
            "Epoch 246/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2773 - g_loss: 0.7521\n",
            "Epoch 247/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2774 - g_loss: 0.7592\n",
            "Epoch 248/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2767 - g_loss: 0.7670\n",
            "Epoch 249/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2770 - g_loss: 0.7757\n",
            "Epoch 250/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2776 - g_loss: 0.7853\n",
            "Epoch 251/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2766 - g_loss: 0.7962\n",
            "Epoch 252/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2768 - g_loss: 0.8083\n",
            "Epoch 253/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2767 - g_loss: 0.8219\n",
            "Epoch 254/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2757 - g_loss: 0.8373\n",
            "Epoch 255/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2757 - g_loss: 0.8546\n",
            "Epoch 256/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2751 - g_loss: 0.8741\n",
            "Epoch 257/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2754 - g_loss: 0.8961\n",
            "Epoch 258/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2742 - g_loss: 0.9209\n",
            "Epoch 259/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2746 - g_loss: 0.9490\n",
            "Epoch 260/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2755 - g_loss: 0.9805\n",
            "Epoch 261/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2736 - g_loss: 1.0160\n",
            "Epoch 262/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2745 - g_loss: 1.0552\n",
            "Epoch 263/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2735 - g_loss: 1.0989\n",
            "Epoch 264/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2730 - g_loss: 1.1472\n",
            "Epoch 265/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2732 - g_loss: 1.2002\n",
            "Epoch 266/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2722 - g_loss: 1.2581\n",
            "Epoch 267/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2730 - g_loss: 1.3205\n",
            "Epoch 268/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2727 - g_loss: 1.3874\n",
            "Epoch 269/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2712 - g_loss: 1.4577\n",
            "Epoch 270/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2709 - g_loss: 1.5299\n",
            "Epoch 271/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2722 - g_loss: 1.6038\n",
            "Epoch 272/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2698 - g_loss: 1.6787\n",
            "Epoch 273/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2705 - g_loss: 1.7517\n",
            "Epoch 274/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2698 - g_loss: 1.8218\n",
            "Epoch 275/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2705 - g_loss: 1.8881\n",
            "Epoch 276/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2693 - g_loss: 1.9517\n",
            "Epoch 277/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2704 - g_loss: 2.0117\n",
            "Epoch 278/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2699 - g_loss: 2.0655\n",
            "Epoch 279/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2690 - g_loss: 2.1162\n",
            "Epoch 280/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2695 - g_loss: 2.1611\n",
            "Epoch 281/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2695 - g_loss: 2.1999\n",
            "Epoch 282/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2690 - g_loss: 2.2351\n",
            "Epoch 283/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2694 - g_loss: 2.2677\n",
            "Epoch 284/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2698 - g_loss: 2.2981\n",
            "Epoch 285/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2686 - g_loss: 2.3262\n",
            "Epoch 286/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2690 - g_loss: 2.3526\n",
            "Epoch 287/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2686 - g_loss: 2.3782\n",
            "Epoch 288/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2682 - g_loss: 2.4008\n",
            "Epoch 289/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2690 - g_loss: 2.4230\n",
            "Epoch 290/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2687 - g_loss: 2.4431\n",
            "Epoch 291/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2696 - g_loss: 2.4639\n",
            "Epoch 292/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.4833\n",
            "Epoch 293/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.5001\n",
            "Epoch 294/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2688 - g_loss: 2.5172\n",
            "Epoch 295/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2687 - g_loss: 2.5363\n",
            "Epoch 296/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2692 - g_loss: 2.5526\n",
            "Epoch 297/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.5649\n",
            "Epoch 298/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2688 - g_loss: 2.5759\n",
            "Epoch 299/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.5870\n",
            "Epoch 300/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.5958\n",
            "Epoch 301/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.6029\n",
            "Epoch 302/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.6095\n",
            "Epoch 303/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2689 - g_loss: 2.6147\n",
            "Epoch 304/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2684 - g_loss: 2.6220\n",
            "Epoch 305/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2685 - g_loss: 2.6336\n",
            "Epoch 306/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2684 - g_loss: 2.6484\n",
            "Epoch 307/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.6672\n",
            "Epoch 308/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2681 - g_loss: 2.6896\n",
            "Epoch 309/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2682 - g_loss: 2.7005\n",
            "Epoch 310/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.7044\n",
            "Epoch 311/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2686 - g_loss: 2.7051\n",
            "Epoch 312/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2688 - g_loss: 2.7047\n",
            "Epoch 313/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2684 - g_loss: 2.7115\n",
            "Epoch 314/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.7182\n",
            "Epoch 315/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.7270\n",
            "Epoch 316/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.7384\n",
            "Epoch 317/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.7495\n",
            "Epoch 318/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.7618\n",
            "Epoch 319/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.7657\n",
            "Epoch 320/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2674 - g_loss: 2.7682\n",
            "Epoch 321/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.7765\n",
            "Epoch 322/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.7785\n",
            "Epoch 323/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2688 - g_loss: 2.7724\n",
            "Epoch 324/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2684 - g_loss: 2.7663\n",
            "Epoch 325/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2686 - g_loss: 2.7774\n",
            "Epoch 326/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2687 - g_loss: 2.7942\n",
            "Epoch 327/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2681 - g_loss: 2.8001\n",
            "Epoch 328/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2673 - g_loss: 2.7984\n",
            "Epoch 329/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.8052\n",
            "Epoch 330/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2682 - g_loss: 2.8169\n",
            "Epoch 331/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.8224\n",
            "Epoch 332/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.8172\n",
            "Epoch 333/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8127\n",
            "Epoch 334/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8191\n",
            "Epoch 335/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.8265\n",
            "Epoch 336/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.8266\n",
            "Epoch 337/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2685 - g_loss: 2.8259\n",
            "Epoch 338/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2681 - g_loss: 2.8312\n",
            "Epoch 339/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8299\n",
            "Epoch 340/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2675 - g_loss: 2.8238\n",
            "Epoch 341/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2676 - g_loss: 2.8169\n",
            "Epoch 342/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2674 - g_loss: 2.8165\n",
            "Epoch 343/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2681 - g_loss: 2.8276\n",
            "Epoch 344/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.8391\n",
            "Epoch 345/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8373\n",
            "Epoch 346/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8404\n",
            "Epoch 347/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2682 - g_loss: 2.8251\n",
            "Epoch 348/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2685 - g_loss: 2.8144\n",
            "Epoch 349/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2671 - g_loss: 2.8257\n",
            "Epoch 350/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2682 - g_loss: 2.8334\n",
            "Epoch 351/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8089\n",
            "Epoch 352/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.7958\n",
            "Epoch 353/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8028\n",
            "Epoch 354/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.8263\n",
            "Epoch 355/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8666\n",
            "Epoch 356/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2676 - g_loss: 2.8612\n",
            "Epoch 357/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8452\n",
            "Epoch 358/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2675 - g_loss: 2.8275\n",
            "Epoch 359/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2673 - g_loss: 2.8145\n",
            "Epoch 360/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2685 - g_loss: 2.8220\n",
            "Epoch 361/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2681 - g_loss: 2.8328\n",
            "Epoch 362/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2673 - g_loss: 2.8336\n",
            "Epoch 363/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8311\n",
            "Epoch 364/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2676 - g_loss: 2.8393\n",
            "Epoch 365/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8528\n",
            "Epoch 366/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2671 - g_loss: 2.8544\n",
            "Epoch 367/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8574\n",
            "Epoch 368/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8415\n",
            "Epoch 369/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2668 - g_loss: 2.8275\n",
            "Epoch 370/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2673 - g_loss: 2.8319\n",
            "Epoch 371/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.8472\n",
            "Epoch 372/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.8410\n",
            "Epoch 373/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.8457\n",
            "Epoch 374/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8461\n",
            "Epoch 375/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2673 - g_loss: 2.8530\n",
            "Epoch 376/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2686 - g_loss: 2.8521\n",
            "Epoch 377/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2689 - g_loss: 2.8475\n",
            "Epoch 378/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8325\n",
            "Epoch 379/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.8133\n",
            "Epoch 380/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.8024\n",
            "Epoch 381/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8055\n",
            "Epoch 382/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8295\n",
            "Epoch 383/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2675 - g_loss: 2.8554\n",
            "Epoch 384/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8647\n",
            "Epoch 385/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2680 - g_loss: 2.8480\n",
            "Epoch 386/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8386\n",
            "Epoch 387/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2668 - g_loss: 2.8341\n",
            "Epoch 388/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8367\n",
            "Epoch 389/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2668 - g_loss: 2.8389\n",
            "Epoch 390/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2666 - g_loss: 2.8400\n",
            "Epoch 391/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.8424\n",
            "Epoch 392/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2679 - g_loss: 2.8507\n",
            "Epoch 393/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.8425\n",
            "Epoch 394/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2676 - g_loss: 2.8225\n",
            "Epoch 395/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.8234\n",
            "Epoch 396/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2675 - g_loss: 2.8073\n",
            "Epoch 397/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2673 - g_loss: 2.7989\n",
            "Epoch 398/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2674 - g_loss: 2.8126\n",
            "Epoch 399/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2670 - g_loss: 2.8315\n",
            "Epoch 400/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2674 - g_loss: 2.8277\n",
            "Epoch 401/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2681 - g_loss: 2.8162\n",
            "Epoch 402/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8266\n",
            "Epoch 403/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2675 - g_loss: 2.8385\n",
            "Epoch 404/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2671 - g_loss: 2.8320\n",
            "Epoch 405/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.8322\n",
            "Epoch 406/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2669 - g_loss: 2.8343\n",
            "Epoch 407/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2672 - g_loss: 2.8236\n",
            "Epoch 408/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2683 - g_loss: 2.8199\n",
            "Epoch 409/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2677 - g_loss: 2.8217\n",
            "Epoch 410/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2678 - g_loss: 2.8431\n",
            "Epoch 411/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2657 - g_loss: 2.8352\n",
            "Epoch 412/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2824 - g_loss: 2.2065\n",
            "Epoch 413/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3069 - g_loss: 0.3798\n",
            "Epoch 414/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3116 - g_loss: 0.2364\n",
            "Epoch 415/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3084 - g_loss: 0.2422\n",
            "Epoch 416/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3034 - g_loss: 0.2784\n",
            "Epoch 417/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3040 - g_loss: 0.3118\n",
            "Epoch 418/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3084 - g_loss: 0.3059\n",
            "Epoch 419/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3052 - g_loss: 0.3762\n",
            "Epoch 420/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3066 - g_loss: 0.3914\n",
            "Epoch 421/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3036 - g_loss: 0.4438\n",
            "Epoch 422/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2984 - g_loss: 0.4712\n",
            "Epoch 423/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2976 - g_loss: 0.4710\n",
            "Epoch 424/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2945 - g_loss: 0.4513\n",
            "Epoch 425/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2962 - g_loss: 0.4150\n",
            "Epoch 426/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2973 - g_loss: 0.3895\n",
            "Epoch 427/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2978 - g_loss: 0.3750\n",
            "Epoch 428/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2982 - g_loss: 0.3673\n",
            "Epoch 429/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2977 - g_loss: 0.3673\n",
            "Epoch 430/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2988 - g_loss: 0.3665\n",
            "Epoch 431/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2994 - g_loss: 0.3607\n",
            "Epoch 432/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3005 - g_loss: 0.3260\n",
            "Epoch 433/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3035 - g_loss: 0.3578\n",
            "Epoch 434/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2965 - g_loss: 0.5604\n",
            "Epoch 435/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3067 - g_loss: 0.4323\n",
            "Epoch 436/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3053 - g_loss: 0.2824\n",
            "Epoch 437/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3001 - g_loss: 0.3232\n",
            "Epoch 438/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2984 - g_loss: 0.3293\n",
            "Epoch 439/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2994 - g_loss: 0.3328\n",
            "Epoch 440/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2984 - g_loss: 0.3339\n",
            "Epoch 441/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2974 - g_loss: 0.3421\n",
            "Epoch 442/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2985 - g_loss: 0.3221\n",
            "Epoch 443/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2980 - g_loss: 0.2930\n",
            "Epoch 444/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2987 - g_loss: 0.2738\n",
            "Epoch 445/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3030 - g_loss: 0.2242\n",
            "Epoch 446/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3121 - g_loss: 0.1272\n",
            "Epoch 447/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3155 - g_loss: 0.0800\n",
            "Epoch 448/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3136 - g_loss: 0.0914\n",
            "Epoch 449/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3113 - g_loss: 0.1115\n",
            "Epoch 450/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3084 - g_loss: 0.1271\n",
            "Epoch 451/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3086 - g_loss: 0.1373\n",
            "Epoch 452/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3072 - g_loss: 0.1405\n",
            "Epoch 453/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3084 - g_loss: 0.1421\n",
            "Epoch 454/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3075 - g_loss: 0.1422\n",
            "Epoch 455/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3085 - g_loss: 0.1406\n",
            "Epoch 456/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3089 - g_loss: 0.1404\n",
            "Epoch 457/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3085 - g_loss: 0.1387\n",
            "Epoch 458/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3087 - g_loss: 0.1401\n",
            "Epoch 459/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3068 - g_loss: 0.1426\n",
            "Epoch 460/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3064 - g_loss: 0.1477\n",
            "Epoch 461/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3061 - g_loss: 0.1538\n",
            "Epoch 462/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3043 - g_loss: 0.1593\n",
            "Epoch 463/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3051 - g_loss: 0.1622\n",
            "Epoch 464/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3054 - g_loss: 0.1627\n",
            "Epoch 465/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3069 - g_loss: 0.1618\n",
            "Epoch 466/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3058 - g_loss: 0.1611\n",
            "Epoch 467/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3067 - g_loss: 0.1574\n",
            "Epoch 468/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3060 - g_loss: 0.1535\n",
            "Epoch 469/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3065 - g_loss: 0.1454\n",
            "Epoch 470/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3064 - g_loss: 0.1428\n",
            "Epoch 471/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3070 - g_loss: 0.1478\n",
            "Epoch 472/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3034 - g_loss: 0.1703\n",
            "Epoch 473/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3023 - g_loss: 0.1864\n",
            "Epoch 474/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3020 - g_loss: 0.1846\n",
            "Epoch 475/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3170 - g_loss: 0.1213\n",
            "Epoch 476/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3149 - g_loss: 0.1604\n",
            "Epoch 477/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3035 - g_loss: 0.2761\n",
            "Epoch 478/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2968 - g_loss: 0.4804\n",
            "Epoch 479/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2929 - g_loss: 0.5991\n",
            "Epoch 480/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2941 - g_loss: 0.6682\n",
            "Epoch 481/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2935 - g_loss: 0.6841\n",
            "Epoch 482/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3011 - g_loss: 0.5248\n",
            "Epoch 483/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3065 - g_loss: 0.1775\n",
            "Epoch 484/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3043 - g_loss: 0.2008\n",
            "Epoch 485/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3022 - g_loss: 0.2052\n",
            "Epoch 486/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3026 - g_loss: 0.2132\n",
            "Epoch 487/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3030 - g_loss: 0.2158\n",
            "Epoch 488/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3013 - g_loss: 0.2288\n",
            "Epoch 489/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3005 - g_loss: 0.2449\n",
            "Epoch 490/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2991 - g_loss: 0.2601\n",
            "Epoch 491/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2990 - g_loss: 0.2556\n",
            "Epoch 492/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2978 - g_loss: 0.2466\n",
            "Epoch 493/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2981 - g_loss: 0.2449\n",
            "Epoch 494/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.2997 - g_loss: 0.2256\n",
            "Epoch 495/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3073 - g_loss: 0.1498\n",
            "Epoch 496/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3140 - g_loss: 0.0926\n",
            "Epoch 497/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3107 - g_loss: 0.1034\n",
            "Epoch 498/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3077 - g_loss: 0.1178\n",
            "Epoch 499/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3067 - g_loss: 0.1228\n",
            "Epoch 500/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3087 - g_loss: 0.1148\n",
            "Epoch 501/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3071 - g_loss: 0.1212\n",
            "Epoch 502/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3051 - g_loss: 0.1393\n",
            "Epoch 503/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3073 - g_loss: 0.1477\n",
            "Epoch 504/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3092 - g_loss: 0.1467\n",
            "Epoch 505/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3101 - g_loss: 0.1456\n",
            "Epoch 506/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3069 - g_loss: 0.1377\n",
            "Epoch 507/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3051 - g_loss: 0.1368\n",
            "Epoch 508/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3046 - g_loss: 0.1424\n",
            "Epoch 509/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3047 - g_loss: 0.1454\n",
            "Epoch 510/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3049 - g_loss: 0.1483\n",
            "Epoch 511/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3056 - g_loss: 0.1483\n",
            "Epoch 512/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3061 - g_loss: 0.1476\n",
            "Epoch 513/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3067 - g_loss: 0.1436\n",
            "Epoch 514/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3075 - g_loss: 0.1327\n",
            "Epoch 515/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3074 - g_loss: 0.1278\n",
            "Epoch 516/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3087 - g_loss: 0.1373\n",
            "Epoch 517/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3111 - g_loss: 0.1458\n",
            "Epoch 518/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3093 - g_loss: 0.1846\n",
            "Epoch 519/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3064 - g_loss: 0.1903\n",
            "Epoch 520/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3059 - g_loss: 0.1268\n",
            "Epoch 521/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3058 - g_loss: 0.1252\n",
            "Epoch 522/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3033 - g_loss: 0.1278\n",
            "Epoch 523/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3036 - g_loss: 0.1458\n",
            "Epoch 524/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3026 - g_loss: 0.1508\n",
            "Epoch 525/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3051 - g_loss: 0.1421\n",
            "Epoch 526/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3122 - g_loss: 0.1302\n",
            "Epoch 527/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3099 - g_loss: 0.1559\n",
            "Epoch 528/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3060 - g_loss: 0.1749\n",
            "Epoch 529/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3051 - g_loss: 0.1297\n",
            "Epoch 530/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3064 - g_loss: 0.1316\n",
            "Epoch 531/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3042 - g_loss: 0.1384\n",
            "Epoch 532/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3065 - g_loss: 0.1378\n",
            "Epoch 533/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3056 - g_loss: 0.1434\n",
            "Epoch 534/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3066 - g_loss: 0.1357\n",
            "Epoch 535/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3070 - g_loss: 0.1402\n",
            "Epoch 536/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3065 - g_loss: 0.1425\n",
            "Epoch 537/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3063 - g_loss: 0.1509\n",
            "Epoch 538/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3059 - g_loss: 0.1634\n",
            "Epoch 539/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3067 - g_loss: 0.1665\n",
            "Epoch 540/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3074 - g_loss: 0.1728\n",
            "Epoch 541/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3060 - g_loss: 0.1692\n",
            "Epoch 542/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3061 - g_loss: 0.1484\n",
            "Epoch 543/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3035 - g_loss: 0.1366\n",
            "Epoch 544/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3024 - g_loss: 0.1426\n",
            "Epoch 545/2000\n",
            "8/8 [==============================] - 22s 3s/step - d_loss: 0.3042 - g_loss: 0.1541\n",
            "Epoch 546/2000\n"
          ]
        }
      ],
      "source": [
        "# Recommend 2000 epochs\n",
        "hist = fashgan.fit(ds, epochs=2000, callbacks=[ModelMonitor()])"
      ],
      "id": "a779dceb-aba6-4bf3-af49-0d32a76dd2f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39c665a1-a4cc-41ac-a08a-2e14ba64e88d"
      },
      "source": [
        "### 4.4 Review Performance"
      ],
      "id": "39c665a1-a4cc-41ac-a08a-2e14ba64e88d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54381e8c-93ee-4022-9df6-24c4356720fe"
      },
      "outputs": [],
      "source": [
        "plt.suptitle('Loss')\n",
        "plt.plot(hist.history['d_loss'], label='d_loss')\n",
        "plt.plot(hist.history['g_loss'], label='g_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "54381e8c-93ee-4022-9df6-24c4356720fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d319a982-7ae5-4754-adcf-b490f17a79d6"
      },
      "source": [
        "# 5. Test Out the Generator"
      ],
      "id": "d319a982-7ae5-4754-adcf-b490f17a79d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
      },
      "source": [
        "### 5.1 Generate Images"
      ],
      "id": "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c46f3d6a-8aa5-40d2-a5ac-67a0606a82f0"
      },
      "outputs": [],
      "source": [
        "generator.load_weights(os.path.join('archive', 'generatormodel.h5'))"
      ],
      "id": "c46f3d6a-8aa5-40d2-a5ac-67a0606a82f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14cde11f-cb26-4ebf-ad04-2c64a54f871e"
      },
      "outputs": [],
      "source": [
        "imgs = generator.predict(tf.random.normal((16, 128, 1)))"
      ],
      "id": "14cde11f-cb26-4ebf-ad04-2c64a54f871e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f745982f-c4d7-451f-91a7-f7c4341cb7b7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\n",
        "for r in range(4):\n",
        "    for c in range(4):\n",
        "        ax[r][c].imshow(imgs[(r+1)*(c+1)-1])"
      ],
      "id": "f745982f-c4d7-451f-91a7-f7c4341cb7b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5137cffa-784d-4076-beef-0a067b86d3aa"
      },
      "source": [
        "### 5.2 Save the Model"
      ],
      "id": "5137cffa-784d-4076-beef-0a067b86d3aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7011d68-ef71-4377-91e2-e26a02fab382"
      },
      "outputs": [],
      "source": [
        "generator.save('generator.h5')\n",
        "discriminator.save('discriminator.h5')"
      ],
      "id": "a7011d68-ef71-4377-91e2-e26a02fab382"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d14c2bd3-a344-4ac1-b2ee-6c90420368e6"
      },
      "outputs": [],
      "source": [],
      "id": "d14c2bd3-a344-4ac1-b2ee-6c90420368e6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "fashgan",
      "language": "python",
      "name": "fashgan"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}